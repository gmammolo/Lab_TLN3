{
  "thread": {
    "uuid": "d34ba013cb6034b2b6e498154709b7e9bae780b7",
    "url": "https://habr.com/ru/companies/mclouds/articles/979246/",
    "site_full": "habr.com",
    "site": "habr.com",
    "site_section": "https://habr.com/ru/rss/articles",
    "site_categories": [
      "tech"
    ],
    "section_title": "Хабр",
    "site_title": "Publications &#x2F; My feed &#x2F; Habr",
    "title": "От первых видеокарт к 5 трлн $ в 2025 году: как NVIDIA стала самой дорогой компанией мира",
    "title_full": "От первых видеокарт к 5 трлн $ в 2025 году: как NVIDIA стала самой дорогой компанией мира",
    "published": "2025-12-25T08:32:00.000+02:00",
    "replies_count": 0,
    "participants_count": 1,
    "site_type": "news",
    "country": "RU",
    "main_image": "https://habrastorage.org/getpro/habr/upload_files/911/278/a9d/911278a9d879579c48e4e5af2b10dee5.png",
    "performance_score": 0,
    "domain_rank": 3667,
    "domain_rank_updated": "2025-12-21T23:00:00.000+02:00",
    "licensing_agency": [],
    "social": {
      "facebook": {
        "likes": 0,
        "comments": 0,
        "shares": 0
      },
      "vk": {
        "shares": 0
      }
    }
  },
  "uuid": "d34ba013cb6034b2b6e498154709b7e9bae780b7",
  "url": "https://habr.com/ru/companies/mclouds/articles/979246/",
  "ord_in_thread": 0,
  "author": "mClouds_editor (mClouds.ru)",
  "published": "2025-12-25T08:32:00.000+02:00",
  "title": "От первых видеокарт к 5 трлн $ в 2025 году: как NVIDIA стала самой дорогой компанией мира",
  "text": "29 октября 2025 года NVIDIA преодолела планку капитализации в 5 трлн $ и стала первой компанией в истории с такой рыночной стоимостью. Всего четыре месяца назад, в июле, компания перешагнула отметку в 4 трлн $ — рост на 1 трлн $ менее чем за полгода.\nСегодня компания контролирует 80–90% рынка ИИ-ускорителей, а ее доля в росте индекса S&P 500 в 2025 году составила почти пятую часть. Разбираемся, как технологический прорыв превратил узкоспециализированного производителя в мирового гиганта за несколько лет.\nОт геймерских видеокарт к научным вычислениям\nНачалось всё 5 апреля 1993 года, когда три инженера — Дженсен Хуанг, Кертис Прим и Крис Малаховски — основали NVIDIA. С самого начала компания сделала ставку на разработку графических ускорителей. Учредители верили, что компьютерная графика изменит мир не только игр, но и многих других отраслей. Однако путь к успеху оказался тернистым.\n1995–1998: первый продукт, первый провал и первый успех\nПервый продукт компании, чип NV1, появился в 1995 году. Он объединял графическое ядро, звуковую подсистему и поддержку игровых контроллеров. Однако инженеры NVIDIA приняли рискованное решение: использовать для 3D-графики квадратичные поверхности вместо полигональной графики, которую продвигал Microsoft DirectX. В результате несовместимость с популярными играми привела к коммерческому провалу. Компания Diamond Multimedia вернула NVIDIA почти все из 250 тысяч заказанных чипов. NVIDIA оказалась на грани банкротства.\nТехнологию пришлось пересмотреть. И в 1997 году компания выпустила Riva 128 — чип, полностью совместимый с DirectX и с лучшей производительностью, чем у конкурентов. Видеокарта Riva 128 имела 128-битную шину памяти, поддерживала разрешение до 1024 × 768 пикселей и могла отображать до 16,7 млн цветов. Это позволило играм работать с частотой 30–40 кадров в секунду в разрешении 800 × 60 — превосходно по меркам 1997 года.\nЗа четыре месяца NVIDIA продала более миллиона чипов Riva 128, что принесло компании 29 млн $ выручки и позволило инвестировать в разработку новых продуктов.\nУспех Riva 128 позволил компании выйти на публичный рынок. 22 января 1999 года NVIDIA провела первичное публичное размещение акций (IPO) на бирже Nasdaq по цене 12 $ за акцию, а рыночная капитализация компании составила 562,8 млн $.\n1999–2001: рождение концепции GPU\nНовым шагом стал выпуск GeForce 256 в 1999 году. NVIDIA позиционировала его как первый в мире графический процессор (GPU), способный самостоятельно выполнять сложные 3D-вычисления, включая трансформации и освещение, без нагрузки на центральный процессор. Главным нововведением стал блок T&L, который аппаратно поддерживал трансформацию вершин 3D-объектов, отсечение и освещение — задачи, ранее выполнявшиеся центральным процессором.\nКомпания S3 Graphics также выпустила чип Savage2000 с аналогичными возможностями всего за день до GeForce 256. Однако маркетинговая стратегия NVIDIA оказалась эффективнее — именно она смогла популяризовать и закрепить в индустрии термин GPU.\nВ 2001 году NVIDIA представила GeForce 3 с шейдерами — небольшими специализированными программами, которые выполняются непосредственно на графическом процессоре. Они позволили разработчикам игр гибко настраивать процесс отрисовки и создавать кинематографические эффекты. Именно шейдеры позже сыграли решающую роль в научном применении видеокарт.\nК концу 2001 года рыночная капитализация NVIDIA выросла до 9,66 млрд $ — почти в 17 раз по сравнению с моментом IPO.\n2005–2007: от игр к науке\nПримерно в середине 2000-х годов ученые из Стэнфордского университета обнаружили, что шейдеры GeForce 6 можно использовать для неграфических вычислений. Они нашли способ «обмануть» видеокарту, заставив ее рассчитывать процессы свертывания белков вместо отображения графики. GPU с их параллельной архитектурой оказались намного эффективнее для определенных типов вычислений, чем традиционные CPU.\nКомпания запустила проект по созданию технологии, которая позволила бы ученым и инженерам легко программировать GPU для неграфических задач. Так в феврале 2007 года появилась CUDA — программно-аппаратная архитектура параллельных вычислений. Первой видеокартой с поддержкой CUDA стала NVIDIA GeForce 8800 GTX. Ее производительность достигала почти 350 гигафлопсов, что в десятки раз превосходило быстродействие многоядерных процессоров того же поколения.\nСтратегический поворот в сторону научных вычислений оправдал себя: к концу 2007 года капитализация компании достигла 18,9 млрд $.\n2008–2012: экспансия в научные вычисления\nАрхитектура CUDA показала себя особенно эффективной в задачах моделирования молекулярной динамики, астрофизики, вычислительной биологии и химии, анализе финансовых данных и многих других областях. По данным специалистов самой NVIDIA и Иллинойского университета, для задач вычислительной биологии CUDA-вычислитель показывал 47-кратное преимущество над современным двухъядерным процессором, а для некоторых финансовых расчетов — почти 200-кратное.\nЧтобы ускорить внедрение CUDA, NVIDIA начала активно сотрудничать с учебными заведениями: компания бесплатно раздавала видеокарты университетам, проводила технологические саммиты и выкладывала в открытый доступ образовательные материалы.\nНесмотря на первоначальный скепсис, NVIDIA продолжала инвестировать в развитие технологии. В 2012 году студент Алекс Крижевский создал нейронную сеть AlexNet, которая с помощью всего двух чипов NVIDIA GTX 580 смогла с высокой точностью распознавать объекты на изображениях. Стало очевидно, что архитектура GPU подходит и для обучения нейронных сетей.\nК концу 2012 года капитализация NVIDIA снизилась до 7,65 млрд $: компания переживала непростые времена из-за снижения спроса на видеокарты, но продолжала инвестировать в будущее.\nЛинейка ИИ-ускорителей: от V100 к Rubin\nКомпания увидела потенциал в создании чипов для машинного обучения и искусственного интеллекта. Проследим ее дальнейшую историю по ускорителям — каждый новый чип отражает этапы развития технологий.\nВыпущенный в 2017 году чип V100 на архитектуре Volta включал 640 специализированных тензорных ядер для операций с матрицами, которые лежат в основе глубокого обучения.\nПроизводительность GPU достигла 125 терафлопсов для операций с низкой точностью. Это был колоссальный скачок в производительности, который сделал V100 стандартом для исследователей искусственного интеллекта и дата-центров.\nВ 2020 году NVIDIA представила A100 на архитектуре Ampere. A100 выполнял до пяти петаопераций в секунду для ИИ-вычислений благодаря тензорным ядрам третьего поколения и новому типу памяти HBM2.\nВажным нововведением в A100 стала технология Multi-Instance GPU (MIG), которая позволяла разделить один физический GPU на несколько изолированных экземпляров для повышения эффективности использования ресурсов в многопользовательских средах. Это сделало A100 не только более мощным, но и более гибким решением для центров обработки данных.\nБлагодаря буму игровой индустрии и росту сегмента ЦОДов к концу 2020 года рыночная стоимость компании достигла 323,24 млрд $. Взлет в 40+ раз за восемь лет!\nВ 2022 году NVIDIA выпустила сразу две важные архитектуры. В марте представили Hopper с флагманским H100, который получил тензорные ядра четвертого поколения и трансформаторный движок для обучения больших языковых моделей (LLM).\nВ сентябре того же года NVIDIA анонсировала архитектуру Ada Lovelace, предназначенную для дата-центров и профессиональных задач. Ключевым продуктом стал ускоритель L40, а затем и его улучшенная версия L40S с 48 ГБ памяти GDDR6. L40S получил улучшенные тензорные ядра четвертого поколения и показал высокую эффективность в задачах инференса нейронных сетей.\nВ отличие от H100, оптимизированного для обучения моделей, L40S обеспечивал оптимальный баланс между производительностью и энергоэффективностью для запуска ИИ-приложений в продакшене, и с заметно меньшей ценой. Серверы на базе L40S, например у нас в mClouds, используются для задач с ИИ, обучением и инференсом нейросетей.\nОднако если в 2021 году капитализация NVIDIA составляла около 735 млрд $, то в 2022 году компания потеряла половину стоимости из-за общего падения технологических акций. Оно было недолгим — уже к маю 2023 года рыночная стоимость NVIDIA впервые перевалила за 1 трлн $.\nВ марте 2024 года NVIDIA показала новое поколение ИИ-ускорителей на архитектуре Blackwell. B100 и B200 созданы по многочиповой архитектуре и объединяют 208 млрд транзисторов. Тензорные ядра пятого поколения обеспечивают производительность до 20 петафлопсов в задачах ИИ, что в 4–5 раз выше показателей H100.\nBlackwell впервые использовал новую технологию трансформерного движка второго поколения, который еще больше ускорил работу с крупными языковыми моделями. Появился также выделенный блок для обработки модуляций Гаусса — это значительно улучшило возможности генерации изображений.\nФлагманская система GB200 объединила два ускорителя B200 и CPU Grace в единую систему с общей памятью и сверхбыстрым соединением, что позволило создавать суперкомпьютеры нового поколения. В 2025 году Министерство энергетики США анонсировало строительство семи таких суперкомпьютеров, крупнейший из которых будет содержать 100 000 процессоров Blackwell.\nАнонс Blackwell помог компании к концу 2024 года достигнуть капитализации в 3,29 трлн $.\nОсенью 2025 года NVIDIA продемонстрировала первые детали об архитектуре Rubin, выход которой запланирован на 2026 год. Монолитный чип Rubin CPX оснащен 128 ГБ памяти GDDR7, может обрабатывать миллионы токенов информации и предназначен для обработки контекста. Процессоры R-100, как ожидается, получат память HBM4 и усовершенствованные тензорные ядра, что должно привести к существенному повышению производительности в задачах генерации.\nПо предварительным данным, Rubin сможет обеспечить до 35-кратного прироста производительности ИИ-инференса по сравнению с ранними моделями Blackwell. Можно будет создавать еще более мощные и интеллектуальные системы ИИ.\nУже в июле 2025 года капитализация NVIDIA составила 4,25 трлн $, а в октябре преодолела отметку в 5 трлн $. Сейчас рыночная стоимость компании скорректировалась до летних отметок. В качестве причин эксперты упоминают фиксацию прибыли, усиление конкуренции и общую волатильность рынка ИИ-технологий.\nКаждое новое поколение ускорителей NVIDIA не только повышает технические характеристики, но и укрепляет экосистему компании. Комплексные программные библиотеки, наборы инструментов для разработчиков, отраслевые решения — вместе это создает преимущество, которое конкурентам становится всё труднее преодолеть.\nБитва за рынок: могут ли конкуренты догнать лидера\nСейчас компания контролирует 80–90% рынка и продолжает наращивать отрыв от конкурентов. Хотя соперники активно работают над собственными линейками видеокарт.\nAMD, главный конкурент NVIDIA на рынке игровых видеокарт, активно развивает линейку Instinct. В 2025 году AMD представила MI355X на новой архитектуре CDNA4. Этот ускоритель получил до 288 ГБ памяти HBM3E. Компания также анонсировала планы по выпуску MI400 с памятью HBM4, но конкретные сроки пока не объявила. Основная проблема AMD — программная платформа ROCm всё еще отстает по функциональности, стабильности и охвату от CUDA. Переход на AMD означает не только замену железа, но и адаптацию всего программного стека, что отпугивает многих потенциальных клиентов.\nIntel также стремится стать значимым игроком на рынке. В 2019 году компания приобрела израильский стартап Habana Labs и начала развивать линейку ускорителей Gaudi. В сентябре 2024 года появился чип Gaudi 3, который оснащен 128 ГБ памяти HBM2e с пропускной способностью 3,7 ТБ/с и обеспечивает производительность 1,8 петафлопса для FP8 и BF16 вычислений. По заявлениям Intel, Gaudi 3 демонстрирует до 1,7 раза более высокую производительность обучения и на 50% лучшую производительность инференса по сравнению с H100. При этом ускорители Gaudi не получили широкого распространения из-за опоздания на рынок и недостаточно развитой программной экосистемы Intel.\nHuawei с линейкой Ascend заслуживает отдельного внимания. Несмотря на санкции, китайская компания продолжает развивать собственные AI-ускорители, которые уже активно используются внутри Китая крупнейшими технологическими гигантами: ByteDance, Baidu, Alibaba и Tencent.\nКитайский рынок AI-чипов огромен и по прогнозам достигнет 50 млрд $ уже в 2025 году. Не случайно администрация Трампа недавно согласовала отгрузки модифицированных чипов H200 в Китай — это попытка притормозить развитие собственных китайских AI-технологий и не дать NVIDIA окончательно потерять свою долю на этом стратегически важном рынке. Подробнее о противостоянии NVIDIA и Huawei можно прочитать в нашей предыдущей статье на Хабре.\nОблачные гиганты также пытаются снизить зависимость от NVIDIA. Они разрабатывают собственные специализированные чипы:\nGoogle с 2013 года развивает Tensor Processing Units (TPU) и в 2025 году представил TPU v7 (Ironwood), начав продавать их сторонним клиентам.\nAmazon Web Services в декабре 2025 года выпустила Trainium3 на 3-нм техпроцессе, который в четыре раза быстрее предыдущего поколения.\nMicrosoft разрабатывает Azure Maia AI Accelerator, получив доступ к полупроводниковым разработкам OpenAI.\nОднако даже эти гиганты не могут полностью отказаться от NVIDIA. Amazon анонсировала, что Trainium4 будет поддерживать NVIDIA NVLink Fusion для совместимости с GPU NVIDIA. Microsoft продолжает расширять партнерство с NVIDIA и AMD, предлагая клиентам виртуальные машины на базе H100 и H200. Попытки создать альтернативы только подчеркивают технологическое доминирование NVIDIA.\nВойдите, пожалуйста.",
  "highlightText": "",
  "highlightTitle": "",
  "highlightThreadTitle": "",
  "language": "russian",
  "sentiment": "positive",
  "categories": [
    "Economy, Business and Finance",
    "Science and Technology"
  ],
  "topics": [
    "Economy, Business and Finance->business and market analysis",
    "Science and Technology->technology and engineering",
    "Science and Technology->scientific innovation",
    "Science and Technology->electronic engineering"
  ],
  "ai_allow": true,
  "has_canonical": false,
  "breaking": false,
  "webz_reporter": false,
  "external_links": [
    "https://mclouds.ru/pricing/gpu-for-ai-ml/",
    "https://mclouds.ru/pricing/gpu-for-ai-ml",
    "https://www.mclouds.ru/pricing/gpu-for-ai-ml/"
  ],
  "external_images": [],
  "internal_images": [],
  "entities": {
    "persons": [],
    "locations": [],
    "organizations": []
  },
  "syndication": {
    "syndicated": null,
    "syndicate_id": null,
    "first_syndicated": false
  },
  "trust": {
    "categories": [],
    "top_news": [],
    "bias": null,
    "source": {
      "type": null,
      "city": null,
      "state": null,
      "country": null,
      "domain_type": null,
      "agency": null,
      "organization_name": null
    }
  },
  "rating": null,
  "crawled": "2025-12-25T08:45:22.355+02:00",
  "updated": "2025-12-25T08:47:39.000+02:00"
}